
\documentclass[5pt,a4paper]{article}
\usepackage{geometry}
\geometry{
	a4paper,
	total={170mm,257mm},
	left=20mm,
	top=20mm,
}
\usepackage[latin1]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\begin{document}
	\title{Machine learning Homework- Deep Learning }
	\author{Abinav Ravi Venkatakrishnan - 03694216 and Abhijeet Parida - 03679676}
	\maketitle
	\section{Activation Function}
	\subsection*{Problem 1:}
	The matrix operation $w^T+b$ is essentially a linear operation. When we stack linear operations over other linear operations we essentially get a linear function. It is impossible to approximate complex functions with just linear operations, therefore non-linearity is introduced to overcome this problem.
	
	\subsection*{Problem 2:}
	The sigmoid activation function is \\
	\begin{equation*}
	\sigma(x)=\frac{1}{1+e^{-x}}
	\end{equation*}
	The tanh activation is\\
	\begin{eqnarray*}
	tanh(x)&=\frac{e^{2x}-1}{e^{2x}+1}\\
	tanh(\frac{x}{2})&=\frac{e^{x}-1}{e^{x}+1}\\
	tanh(\frac{x}{2})&=\frac{1-e^{-x}}{1+e^{-x}}\\
	tanh(\frac{x}{2})&=(1-e^{-x})\sigma(x)\\
	\end{eqnarray*}
	
	\subsection*{Problem 3:}
	From the previous problem we know that,
	\begin{eqnarray*}
		tanh(x)&=\frac{e^{2x}-1}{e^{2x}+1}\\
		tanh(x)&=\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}\\
	\end{eqnarray*}
	Taking derivative on both sides we get,
	\begin{eqnarray*}
		\frac{d}{dx}(tanh(x))&=\frac{d}{dx}(\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}})\\
		&=(e^{x}+e^{-x})(e^{x}+e^{-x})^{-1}-(e^{x}+e^{-x})^{-2}(e^{x}-e^{-x})(e^{x}-e^{-x})\\
		&= 1- (\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}})^2
		\frac{d}{dx}(tanh(x))&=1-(tanh(x))^2
	\end{eqnarray*}
	The advantage is that it is easy to compute the gradients during backpropagation.
	\section{ Numerical Stability}
	\subsection*{Problem 4:}
	
	\subsection*{Problem 5:}
	\subsection*{Problem 6:}
	
	
\end{document}