\documentclass[5pt,a4paper]{article}
\usepackage{geometry}
\geometry{
	a4paper,
	total={170mm,257mm},
	left=20mm,
	top=20mm,
}
\usepackage[latin1]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}

\begin{document}
	\title{Machine Learning Homework 2}
	\author{Abinav Ravi Venkatakrishnan \& Abhijeet Parida}
	
	\maketitle
	
	\section{Optimising Likelihood: Monotonic Transforms}
	\subsection*{Problem 1}
	$f=\theta^t(1-\theta)^h$\\
	First Derivative:\\
	$\frac{df}{d \theta}=t(1-\theta)^h\theta^{t-1}- h \theta^t(1-\theta)^{h-1}$\\
	Second Derivative:\\
	$\frac{d}{d \theta}\frac{df}{d \theta}=ht\theta^{t-1}(1-\theta)^{h-1}+t(t-1)\theta^{t-2}(1-\theta)^h-\{ht \theta^{t-1}(1-\theta)^{h-1}+h(h-1)\theta^t(1-\theta)^{h-2}\}$\\\\
	$f=log(\theta^t(1-\theta)^h)$\\
	$f=tlog(\theta)+h log(1-\theta)$\\
	First Derivative:\\
	$\frac{df}{d \theta}=\frac{t}{\theta}+\frac{h}{1-\theta}$\\
	Second Derivative:\\
	$\frac{d}{d \theta}\frac{df}{d \theta}=\frac{-t}{\theta^2}+\frac{h}{(1-\theta)^2}$
	\subsection*{Problem 2}
	Let local maxima for any positive differentiable function $f(\theta)$ be $\theta^*$. Then $\theta^*$ satisfies the condition: 
	\begin{enumerate}
		\item $\frac{d}{d\theta}f(\theta^*)=0$\label{condition1}
		\item $\frac{d^2}{d\theta^2}f(\theta^*)<0$ \label{condition2}
	\end{enumerate}

For the function $log (f(\theta))$, we find the first derivative and equate to zero,
\begin{equation*}
\frac{d }{d \theta}log(f(\theta))= \frac{1}{f(\theta)}\frac{d}{d\theta}f(\theta)
\end{equation*}
For a positive function $f(\theta)$ is positive and from \ref{condition1}, $\frac{d}{d\theta}f(\theta)= 0$ at $\theta^*$. Also, the second derivative of $log (f(\theta))$ is  
\begin{equation*}
\frac{d^2 }{d \theta^2}log(f(\theta))= -\frac{1}{f(\theta)^2}\frac{d}{d\theta} f(\theta) + \frac{1}{ f(\theta)}\frac{d^2}{d \theta^2} f(\theta)
\end{equation*}
 In the above equation we can notice that the first term is negative and from \ref{condition2} second term is also negative hence proving that $\theta^*$ is a local maximum of both $f(\theta)$ and $log(f(\theta))$\\
	
From the two problem we understand that Monotonic functions preserve the critical points. Conversion of the problem to a log scale makes the computation easy and is preferred. 

\section{Properties of MLE and MAP}
\subsection*{Problem 3} 
 from Bayes rule we know that 
 \begin{equation}
 p(\theta |D) = \frac{p(D|\theta)p(\theta)}{p(D)}
 \end{equation}
 now we know that $p(\theta |D)$ is the prior estimate $\theta_{MAP}$ and $p(D|\theta)$ is the Maximum Likelihood Estimate $\theta_{MLE}$ so we get the relation 
 \begin{equation*}
 	posterior \propto likelihood \text{ x } prior
 \end{equation*}
for a uniform prior $p(\theta)$ we can say that $\theta_{MLE}$ is a special case of $\theta_{MAP}$

\subsection*{Problem 4}
\begin{equation*}
posterior \propto likelihood \text{ x } prior
\end{equation*}
The prior is beta distribution which is conjugate prior for the binomial distribution of the likelihood. Therefore, from lectures the distribution of the posterior is same as the prior distribution.

\begin{equation*}
posterior \propto \text{ }^NC_m \theta^m(1-\theta)^{N-m} \text{ * } \frac{\Gamma(a+b)}{\Gamma(a)+\Gamma(b)}\theta^{a-1}(1-\theta)^{N-m}
\end{equation*}
by simple pattern matching with Beta distribution we get posterior mean as\\
\begin{equation}
\mathop{\mathbb{E}}[\theta|\mathcal{D}]=\frac{m+a}{m+l+a+b}\label{posterior}
\end{equation}
From the mean of the beta distribution we get 
\begin{equation}
\mathop{\mathbb{E}}[p(\theta)]=\frac{a}{a+b}
\end{equation}
Also $\theta_{MLE}$
From the Eq. \ref{posterior}
\begin{eqnarray}
\frac{m+a}{m+l+a+b}&=\frac{m}{m+l+a+b}+\frac{a}{m+l+a+b}\\
&=\frac{m}{m+l}\frac{m+l}{m+l+a+b}+\frac{a}{a+b}\frac{a+b}{m+l+a+b}\\
&=(1-\lambda)\frac{m}{m+l}+\lambda \frac{a}{a+b}
\end{eqnarray}
Hence the posterior mean value $\mathop{\mathbb{E}}[\theta|\mathcal{D}]$ lies between prior mean of $\theta$ and the $\theta_{MLE}$.
	
\section{Poisson Distribution}
\subsection*{Problem 5}
\subsubsection*{Part a}
The $\mathop{\mathbb{E}}[\lambda]$ is the mean of the Poisson distribution which is $\lambda$, which gives the maximum likelihood estimate.

\subsubsection*{Part b}
It is known that the conjugate prior of the Poisson distribution is $Gamma(\alpha,\beta)$. Therefore our posterior is also $Gamma(\alpha,\beta)$.\\
So,
\begin{eqnarray*}
p(\lambda|\mathcal{D}) & \propto p(\mathcal{D}|\lambda)\text{ * } p(\lambda) \\
&=e^{(-n \lambda)} \prod_{i=1}^n \frac{\lambda^{k_i}}{k_i!}e^{-\beta \lambda}\\
& \propto \lambda^{\sum_{i=1}^{n}k_i+\alpha-1}e^{-n\lambda-\beta\lambda}
\end{eqnarray*}
By simple pattern matching the mode gamma distribution, we get
\begin{equation}
\lambda=\frac{\sum_{i=1}^{n}k_i+\alpha -1}{n+\beta}
\end{equation}


	
	
\end{document}