
\documentclass[5pt,a4paper]{article}
\usepackage{geometry}
\geometry{
	a4paper,
	total={170mm,257mm},
	left=20mm,
	top=20mm,
}
\usepackage[latin1]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\begin{document}
	\title{Machine learning Homework- Soft-Margin SVM and Kernels }
	\author{Abinav Ravi Venkatakrishnan - 03694216 and Abhijeet Parida - 03679676}
	\maketitle
	\section*{Problem 1:}
	No it will not be the correct label. The training sample depends on the distance from the hyperplane decision boundary $\xi$. If $ \xi < 1 $ for the training sample it gets classified correctly else it gets mis-classified.
	
	\section*{Problem 2:}
	The cost function for soft-margin SVM is 
	\begin{equation}
	min f_0(\textbf{w},b,\xi) = \frac{1}{2}\textbf{w}^T \textbf{w}+ C \sum_{i=1}^{N} \xi_i
	\end{equation}
	C is a penalizing factor on $ \xi $. \\
	case 1: when C = 0 there is no restriction on $\xi$ values.\\
	case 2: when C < 0 it encourages higher values of $\xi$ and hence encouraging mis-classification.  
	\section*{Problem 3:}
	
	
	\section*{Problem 4:}
	
	\section*{Problem 5:}
	\begin{enumerate}
		\item from $g(\alpha)$ we know that $\alpha Q\alpha^T$ is equivalent to $-\sum_{i=1}^{N}\sum_{j=1}^{N} y_i y_j \alpha_i \alpha_j \textbf{x}_i \textbf{x}_j$. By rearranging the scalars we get,$-\sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_i  y_i y_j  \textbf{x}_i \textbf{x}_j \alpha_j$. \\Therefore $Q= (-yy^T(hadamard)XX^T)$		\item We know that $Q=-p^Tp$ and also we know that $p^Tp$ is positive semi definite due to its symmetric nature. So $a^t (p^Tp)a \geq 0$ but we negative sign also. Therefore, $Q$ is negative semi definite.
		\item The negative semi definiteness allows the concave optimisation to be a maximisation problem. 
	\end{enumerate}
	
\end{document}